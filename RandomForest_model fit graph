
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import pandas as pd
import matplotlib.pyplot as plt


# Load the cleaned dataset
df = pd.read_csv("cleanedHDB.csv")

# Data preprocessing and feature extraction
df['month'] = pd.to_datetime(df['month'])

df[["storey_min", "storey_max"]] = df["storey_range"].str.split(" TO ", expand=True).astype(int)
df["storey_avg"] = (df["storey_min"] + df["storey_max"]) / 2

def categorize_storey(storey_avg):
    if storey_avg <= 4:
        return "Low"
    elif storey_avg <= 8:
        return "Middle"
    else:
        return "High"

df["storey_category"] = df["storey_avg"].apply(categorize_storey)

# One-Hot Encoding for categorical variables
encoder = OneHotEncoder(sparse_output=False, drop='first')
one_hot_encoded = encoder.fit_transform(df[['town', 'estate_type', 'storey_category']])
one_hot_columns = encoder.get_feature_names_out(['town', 'estate_type', 'storey_category'])
df_encoded = pd.DataFrame(one_hot_encoded, columns=one_hot_columns)

# Concatenate the one-hot encoded columns with the original DataFrame
df = pd.concat([df, df_encoded], axis=1).drop(columns=['town', 'estate_type', 'storey_category'])

# Features and target selection
features = ["remaining_years", "floor_area_sqm", "mrt_station"] + list(one_hot_columns)
target = "resale_price"

df = df[features + [target] + ['month']]

# Remove outliers
upper_limit = df['resale_price'].quantile(0.99)
df = df[df['resale_price'] <= upper_limit]

# Prepare feature matrix (X) and target vector (y)
X = df.drop(columns=[target, 'month'])
y = df[target]

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# StandardScaler to scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize RandomForestRegressor
reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)

# Train the model with the scaled training data
reg.fit(X_train_scaled, y_train)
y_predict = reg.predict(X_test_scaled)

# performance metrics
r2_train = r2_score(y_train, reg.predict(X_train_scaled))
r2_test = r2_score(y_test, y_predict)

mae_train = mean_absolute_error(y_train, reg.predict(X_train_scaled))
mae_test = mean_absolute_error(y_test, y_predict)

mse_train = mean_squared_error(y_train, reg.predict(X_train_scaled))
mse_test = mean_squared_error(y_test, y_predict)


# Visualizing overfitting and underfitting using three metrics
plt.figure(figsize=(12, 6))

# Subplot for Training Data
plt.subplot(1, 2, 1)
plt.scatter(y_train, reg.predict(X_train_scaled), color='blue', label='Train Data')
plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', linestyle='--', label="Perfect Fit")
plt.title('Training Data: Underfit, Overfit or Goodfit')
plt.xlabel('Actual Resale Price')
plt.ylabel('Predicted Resale Price')
plt.legend()
plt.grid(True)

# Subplot for Test Data
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_predict, color='green', label='Test Data')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label="Perfect Fit")
plt.title('Testing Data: Underfit, Overfit or Goodfit')
plt.xlabel('Actual Resale Price')
plt.ylabel('Predicted Resale Price')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Print model status based on MAE and RÂ²
if mae_train < mae_test and r2_train > 0.9 and r2_test < 0.5:
    print("The model is overfitting.")
elif mae_train > mae_test and r2_train < 0.5 and r2_test < 0.5:
    print("The model is underfitting.")
else:
    print("The model is a good fit.")

